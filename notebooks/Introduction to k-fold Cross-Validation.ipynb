{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to k-fold Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Written by [*kvs setty*](https://kvssetty.com/) 20th Aug 2020 in [Data Science](https://kvssetty.com/category/data-science/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross-validation is a statistical method used to estimate the performance of machine learning models.\n",
    "\n",
    "It is commonly used in applied machine learning to compare and select a model for a given predictive modeling problem because it is easy to understand, easy to implement, and results in performance estimates that generally have a lower standard error/bias than other methods.\n",
    "\n",
    "In this tutorial, you will discover a gentle introduction to the **k-fold cross-validation** procedure for estimating the performance of machine learning models.\n",
    "\n",
    "After completing this tutorial, you will know:\n",
    "\n",
    "* That k-fold cross validation is a procedure used to estimate the performance of the model on new data (data not used during training a.k.a unseen data/hold-out data).\n",
    "* There are common approaches that you can use to select the value of k (number of folds) for your dataset.\n",
    "* There are commonly used variations on cross-validation such as stratified and repeated that are available in scikit-learn.\n",
    "\n",
    "**Note:** you can learn more about statistical hypothesis testing, resampling methods, estimation statistics, nonparametric methods by signing in to my [week-end online classes](https://www.mlanddlguru.com/b/) or step-by-step tutorials and full source code from my [blog](https://kvssetty.com/)\n",
    "\n",
    "Let’s get started."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![random-sampling](../images/analytics.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial Overview\n",
    "This tutorial is divided into 5 parts; they are:\n",
    "\n",
    "* k-Fold Cross-Validation\n",
    "* Configuration of k\n",
    "* Worked Example\n",
    "* Cross-Validation API\n",
    "* Variations on Cross-Validation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k-Fold Cross-Validation\n",
    "Cross-validation is a resampling procedure used to evaluate machine learning models when you have a limited data sample.\n",
    "\n",
    "The procedure has a single parameter called k that refers to the number of groups that a given data sample is to be split into. As such, the procedure is often called k-fold cross-validation. When a specific value for k is chosen, it may be used in place of k in the reference to the model, such as k=10 becoming 10-fold cross-validation.In the same tone when k=5 becoming 5-fold cross validation\n",
    "\n",
    "Cross-validation is primarily used in applied machine learning to estimate the performance of a machine learning model on unseen data. That is, to use a limited sample in order to estimate how the model is expected to perform in general when used to make predictions on data not used during the training of the model.\n",
    "\n",
    "It is a popular method because it is simple to understand and because it generally results in a less biased or less optimistic estimate of the model performance than other methods, such as a simple train/test split.\n",
    "\n",
    "The general procedure is as follows:\n",
    "\n",
    "1. Shuffle the dataset randomly.\n",
    "2. Split the dataset into k groups\n",
    "3. For each unique group:\n",
    "   1. Take the group as a hold out or test data set\n",
    "   2. Take the remaining groups as a training data set\n",
    "   3. Fit a model on the training set and evaluate it on the test set\n",
    "   4. Retain the evaluation score and discard the model\n",
    "4. summarize the performance of the model using the sample of model evaluation scores.\n",
    "\n",
    "Importantly, each observation in the data sample is assigned to an individual group and stays in that group for the duration of the procedure. This means that each sample is given the opportunity to be used in the hold out set 1 time and used to train the model k-1 times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![k-fold-illustration](../images/k-fold-illustration.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The illustartion above gives some idea about the concept of k-fold cross validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">This approach involves randomly dividing the set of observations into k groups, or folds, of approximately equal size. The first fold is treated as a validation set, and the method is fit on the remaining k − 1 folds.\n",
    "\n",
    "— Page 181, [An Introduction to Statistical Learning](https://www.amazon.in/Introduction-Statistical-Learning-Applications-Statistics/dp/1461471370/ref=sr_1_fkmr2_1?dchild=1&keywords=An+Introduction+to+Statistical+Learning%3A+with+Applications+in+R+%28Springer+Texts+in+Statistics%29+1st+ed.+2013%2C+Corr.+7th+printing+2017+Edition&qid=1597906764&s=books&sr=1-1-fkmr2), 2013."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also important that any preparation of the data prior to fitting the model occur on the CV-assigned training dataset within the loop rather than on the broader data set. This also applies to any tuning of hyperparameters. A failure to perform these operations within the loop may result in [data leakage](https://nbviewer.jupyter.org/github/KVSSetty/ml-notebooks/blob/master/How%20to%20Avoid%20Data%20Leakage%20When%20Performing%20Data%20Preparation/How%20to%20Avoid%20Data%20Leakage%20When%20Performing%20Data%20Preparation.ipynb) and an optimistic estimate of the model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Despite the best efforts of statistical methodologists, users frequently invalidate their results by inadvertently peeking at the test data.\n",
    "\n",
    "— Page 708, [Artificial Intelligence: A Modern Approach (3rd Edition)](https://www.amazon.in/Artificial-Intelligence-3e-Modern-Approach/dp/9332543518/ref=sr_1_1?dchild=1&keywords=Artificial+Intelligence%3A+A+Modern+Approach+%283rd+Edition%29+3rd+Edition&qid=1597908415&s=books&sr=1-1), 2009."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**side note:** I am currently reading this book , a heavy book with appx 1150 pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of a k-fold cross-validation run are often summarized with the mean of the model performance scores. It is also good practice to include a measure of the variance of the performance scores, such as the standard deviation or standard error.see the example section below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration of k\n",
    "The k value must be chosen carefully for your data sample.\n",
    "\n",
    "A poorly chosen value for k may result in a mis-representative idea of the performance of the model, such as a score with a high variance (that may change a lot based on the data used to fit the model), or a high bias, (such as an overestimate of the skill of the model).\n",
    "\n",
    "Three common tactics for choosing a value for k are as follows:\n",
    "\n",
    "* **Representative:** The value for k is chosen such that each train/test group of data samples is large enough to be statistically representative of the broader dataset.\n",
    "* **k=10:** The value for k is fixed to 10, a value that has been found through experimentation to generally result in a model skill estimate with low bias a modest variance.\n",
    "* **k=n:** The value for k is fixed to n, where n is the size of the dataset to give each test sample an opportunity to be used in the hold out dataset. This approach is called [leave-one-out cross-validation.](https://kvssetty.com/loocv-for-evaluating-machine-learning-algorithms/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">The choice of k is usually 5 or 10, but there is no formal rule. As k gets larger, the difference in size between the training set and the resampling subsets gets smaller. As this difference decreases, the bias of the technique becomes smaller\n",
    "\n",
    "— Page 70, [Applied Predictive Modeling](https://www.amazon.in/Applied-Predictive-Modeling-Max-Kuhn/dp/1461468485/ref=sr_1_1?dchild=1&keywords=applied+predictive+modeling&qid=1597909537&s=books&sr=1-1), 2013.\n",
    "\n",
    "A value of k=10 or 5 is very common in the field of applied machine learning, and is recommended if you are struggling to choose a value for your dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">To summarize, there is a bias-variance trade-off associated with the choice of k in k-fold cross-validation. Typically, given these considerations, one performs k-fold cross-validation using k = 5 or k = 10, as these values have been shown empirically to yield test error rate estimates that suffer neither from excessively high bias nor from very high variance.\n",
    "\n",
    "— Page 184, [An Introduction to Statistical Learning](https://www.amazon.in/Introduction-Statistical-Learning-Applications-Statistics/dp/1461471370/ref=sr_1_fkmr2_1?dchild=1&keywords=An+Introduction+to+Statistical+Learning%3A+with+Applications+in+R+%28Springer+Texts+in+Statistics%29+1st+ed.+2013%2C+Corr.+7th+printing+2017+Edition&qid=1597909887&s=books&sr=1-1-fkmr2), 2013.\n",
    "\n",
    "If a value for k is chosen that does not evenly split the data sample, then one group will contain a remainder of the examples. It is preferable to split the data sample into k groups with the same number of samples, such that the sample of model performance scores are all equivalent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Worked Example\n",
    "To make the cross-validation procedure concrete, let’s look at a worked example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make the cross-validation procedure concrete, let’s look at a worked example.\n",
    "\n",
    "Imagine we have a data sample with 9 observations:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "...\n",
    "[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to pick a value for k in order to determine the number of folds used to split the data. Here, we will use a value of k=3. That means we will shuffle the data and then split the data into 3 groups. Because we have 9 observations, each group will have an equal number of 3 observations.\n",
    "\n",
    "For example:\n",
    "\n",
    "```python\n",
    "...\n",
    "Fold1: [0.5, 0.2, 0.9]\n",
    "Fold2: [0.1, 0.3, 0.7]\n",
    "Fold3: [0.4, 0.6, 0.8]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then make use of the sample, such as to evaluate the performance of a machine learning algorithm.\n",
    "\n",
    "Three models are trained and evaluated with each fold given a chance to be the held out test set.\n",
    "\n",
    "For example:\n",
    "\n",
    "**Model1:** Trained on Fold1 + Fold2, Tested on Fold3\n",
    "**Model2:** Trained on Fold2 + Fold3, Tested on Fold1\n",
    "**Model3:** Trained on Fold1 + Fold3, Tested on Fold2\n",
    "The models are then discarded after they are evaluated as they have served their purpose.\n",
    "\n",
    "The performance scores are collected for each model and summarized for use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Validation API\n",
    "We do not have to implement k-fold cross-validation manually. The scikit-learn library provides an implementation that will split a given data sample up.\n",
    "\n",
    "The `KFold()` scikit-learn class can be used. It takes as arguments the number of splits, whether or not to shuffle the sample, and the seed for the pseudorandom number generator used prior to the shuffle.\n",
    "\n",
    "For example, we can create an instance that splits a dataset into 3 folds, shuffles prior to the split, and uses a value of 26 for the pseudorandom number generator (for this you can choose any value)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "...\n",
    "kfold = KFold(3, True, 1)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `split()` function can then be called on the class where the data sample is provided as an argument. Called repeatedly, the split will return each group of train and test sets. Specifically, arrays are returned containing the indexes into the original data sample of observations to use for train and test sets on each iteration.\n",
    "\n",
    "For example, we can enumerate the splits of the indices for a data sample using the created KFold instance as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "...\n",
    "# enumerate splits\n",
    "for train, test in kfold.split(data):\n",
    "    print('train: %s, test: %s' % (train, test))\n",
    "```    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can tie all of this together with our small dataset used in the worked example of the prior section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: [0.1 0.2 0.3 0.6 0.7 0.8], test: [0.4 0.5 0.9]\n",
      "train: [0.4 0.5 0.6 0.7 0.8 0.9], test: [0.1 0.2 0.3]\n",
      "train: [0.1 0.2 0.3 0.4 0.5 0.9], test: [0.6 0.7 0.8]\n"
     ]
    }
   ],
   "source": [
    "# scikit-learn k-fold cross-validation\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "# data sample\n",
    "data = np.array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9])\n",
    "# prepare cross validation\n",
    "kfold = KFold(3, True, 26)\n",
    "# enumerate splits\n",
    "for train, test in kfold.split(data):\n",
    "    print('train: %s, test: %s' % (data[train], data[test]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the example above prints the specific observations chosen for each train and test set. The indices are used directly on the original data array to retrieve the observation values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usefully, the k-fold cross validation implementation in scikit-learn is provided as a component operation within broader methods, such as grid-searching model hyperparameters and scoring a model on a dataset.\n",
    "\n",
    "Nevertheless, the KFold class can be used directly in order to split up a dataset prior to modeling such that all models will use the same data splits. This is especially helpful if you are working with very large data samples. The use of the same splits across algorithms can have benefits for statistical tests that you may wish to perform on the data later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variations on Cross-Validation\n",
    "There are a number of variations on the k-fold cross validation procedure.\n",
    "\n",
    "Three commonly used variations are as follows:\n",
    "\n",
    "* **Train/Test Split:** Taken to one extreme, k may be set to 2 (not 1) such that a single train/test split is created to evaluate the model.\n",
    "* **LOOCV:** Taken to another extreme, k may be set to the total number of observations in the dataset such that each observation is given a chance to be the held out of the dataset. This is called leave-one-out cross-validation, or LOOCV for short.\n",
    "* **Stratified:** The splitting of data into folds may be governed by criteria such as ensuring that each fold has the same proportion of observations with a given categorical value, such as the class outcome value. This is called stratified cross-validation.\n",
    "* **Repeated:** This is where the k-fold cross-validation procedure is repeated n times, where importantly, the data sample is shuffled prior to each repetition, which results in a different split of the sample.\n",
    "* **Nested:** This is where k-fold cross-validation is performed within each fold of cross-validation, often to perform hyperparameter tuning during model evaluation. This is called nested cross-validation or double cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## exercise:\n",
    "This section lists some ideas for extending the tutorial that you may wish to explore.\n",
    "\n",
    "- Find 3 machine learning research papers that use a value of 10 for k-fold cross-validation.\n",
    "- Write your own function to split a data sample using k-fold cross-validation.\n",
    "- Develop examples to demonstrate each of the main types of cross-validation supported by scikit-learn.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Reading\n",
    "This section provides more resources on the topic if you are looking to go deeper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Books\n",
    "[Applied Predictive Modeling](https://www.amazon.in/Applied-Predictive-Modeling-Max-Kuhn/dp/1461468485/ref=sr_1_1?dchild=1&keywords=applied+predictive+modeling&qid=1597909537&s=books&sr=1-1), 2013.\n",
    "\n",
    "[An Introduction to Statistical Learning](https://www.amazon.in/Introduction-Statistical-Learning-Applications-Statistics/dp/1461471370/ref=sr_1_fkmr2_1?dchild=1&keywords=An+Introduction+to+Statistical+Learning%3A+with+Applications+in+R+%28Springer+Texts+in+Statistics%29+1st+ed.+2013%2C+Corr.+7th+printing+2017+Edition&qid=1597909887&s=books&sr=1-1-fkmr2), 2013.\n",
    "\n",
    "[Artificial Intelligence: A Modern Approach (3rd Edition)](https://www.amazon.in/Artificial-Intelligence-3e-Modern-Approach/dp/9332543518/ref=sr_1_1?dchild=1&keywords=Artificial+Intelligence%3A+A+Modern+Approach+%283rd+Edition%29+3rd+Edition&qid=1597908415&s=books&sr=1-1), 2009."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### API\n",
    "[sklearn.model_selection.KFold() API](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html)\n",
    "\n",
    "[sklearn.model_selection: Model Selection API](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.model_selection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Articles\n",
    "[Resampling (statistics) on Wikipedia](https://en.wikipedia.org/wiki/Resampling_(statistics))\n",
    "\n",
    "[Cross-validation (statistics) on Wikipedia](https://en.wikipedia.org/wiki/Cross-validation_(statistics))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "In this tutorial, you discovered a basic introduction to the k-fold cross-validation procedure for estimating the performance of machine learning models.\n",
    "\n",
    "Specifically, you learned:\n",
    "\n",
    "* That k-fold cross validation is a procedure used to estimate the skill of the model on new data.\n",
    "* There are common trics that you can use to select the value of k for your dataset.\n",
    "* There are commonly used variations on cross-validation, such as stratified and repeated, that are available in scikit-learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Do you have any questions?**\n",
    "\n",
    "Ask your questions in the comments/response section and I will try my best to answer\n",
    "or still better option is join my [week-end online classes](https://www.mlanddlguru.com/b), which are running free for the time being (you will receive an e-mail for joining-link and timings)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc-autonumbering": true,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
